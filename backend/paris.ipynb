{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\calar\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: flask-cors in c:\\users\\calar\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from flask) (3.0.6)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\calar\\anaconda3\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\calar\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 1: Imports et setup\n",
    "################################################################################\n",
    "\n",
    "# %pip install nltk\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "import os\n",
    "\n",
    "print(\"Imports done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\calar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérifie que tu as téléchargé les corpora NLTK (tokenizers, taggers, etc.).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 2: Téléchargement des ressources NLTK (si besoin)\n",
    "################################################################################\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"Vérifie que tu as téléchargé les corpora NLTK (tokenizers, taggers, etc.).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity_name(text):\n",
    "    return \" \".join(word.capitalize() for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_relation(relation_text):\n",
    "    # Liste des verbes communs pour les relations\n",
    "    common_verbs = {\"was\", \"is\", \"lived\", \"died\", \"born\", \"located\", \"founded\"}\n",
    "    words = relation_text.lower().split()\n",
    "    return any(verb in words for verb in common_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entity(doc, text):\n",
    "    for ent in doc.ents:\n",
    "        if ent.text in text:\n",
    "            return ent.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_and_tag(text):\n",
    "    # Utiliser spaCy pour une meilleure analyse\n",
    "    doc = nlp(text)\n",
    "    # Convertir au format nltk pour compatibilité\n",
    "    tagged = [(token.text, token.pos_) for token in doc]\n",
    "    return tagged\n",
    "\n",
    "# Améliorer la grammaire pour capturer plus de cas\n",
    "GRAMMAR = r\"\"\"\n",
    "    NP: {<DT|PRP\\$>?<JJ.*>*<NN.*>+}\n",
    "        {<NNP>+}\n",
    "        {<NN>+}\n",
    "    VP: {<VB.*>}\n",
    "    RELATION: {<VP><IN|TO>*}\n",
    "\"\"\"\n",
    "chunk_parser = RegexpParser(GRAMMAR)\n",
    "\n",
    "def extract_np_phrases(tagged_text):\n",
    "    tree = chunk_parser.parse(tagged_text)\n",
    "    nps = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "            # Nettoyer et normaliser le texte\n",
    "            np_text = \" \".join([token.lower() for (token, pos) in subtree.leaves()])\n",
    "            np_text = np_text.strip()\n",
    "            if np_text:  # Vérifier que ce n'est pas vide\n",
    "                nps.append(np_text)\n",
    "    return nps\n",
    "\n",
    "def extract_triplet_simple(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extraire le sujet (premier nom propre ou groupe nominal)\n",
    "    subject = None\n",
    "    for chunk in doc.noun_chunks:\n",
    "        subject = get_named_entity(doc, chunk.text)\n",
    "        break\n",
    "    \n",
    "    # Extraire l'objet (dernier nom propre ou groupe nominal)\n",
    "    object_ = None\n",
    "    for chunk in doc.noun_chunks:\n",
    "        object_ = get_named_entity(doc, chunk.text)\n",
    "    \n",
    "    if not (subject and object_) or subject == object_:\n",
    "        return None\n",
    "    \n",
    "    # Normaliser les entités\n",
    "    subject = normalize_entity_name(subject)\n",
    "    object_ = normalize_entity_name(object_)\n",
    "    \n",
    "    # Extraire la relation\n",
    "    relation = []\n",
    "    subject_token = None\n",
    "    object_token = None\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in subject:\n",
    "            subject_token = token\n",
    "        if token.text in object_:\n",
    "            object_token = token\n",
    "            \n",
    "    if subject_token and object_token:\n",
    "        start_idx = min(subject_token.i, object_token.i)\n",
    "        end_idx = max(subject_token.i, object_token.i)\n",
    "        \n",
    "        for token in doc[start_idx:end_idx]:\n",
    "            if token.pos_ in ['VERB', 'ADP', 'PART']:\n",
    "                relation.append(token.text)\n",
    "    \n",
    "    relation_text = \" \".join(relation).strip()\n",
    "    \n",
    "    if not relation_text or not validate_relation(relation_text):\n",
    "        return None\n",
    "        \n",
    "    return (subject, relation_text, object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON chargé. Nombre de ressources: 64912\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 4: Charger le JSON local\n",
    "################################################################################\n",
    "\n",
    "json_path = r\"C:\\Users\\calar\\OneDrive\\Bureau\\M2\\NLP\\NLP PROJECT\\backend\\sparql_2025-01-23_17-04-11Z.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "print(\"JSON chargé. Nombre de ressources:\", len(data_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy fuzzywuzzy python-Levenshtein requests\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "RELATION_MAP = {\n",
    "    \"was born in\": [\n",
    "        \"http://dbpedia.org/property/birthPlace\",\n",
    "        \"http://dbpedia.org/ontology/birthPlace\"\n",
    "    ],\n",
    "    \"lived in\": [\n",
    "        \"http://dbpedia.org/ontology/residence\",\n",
    "        \"http://dbpedia.org/property/residence\",\n",
    "        \"http://dbpedia.org/ontology/livingPlace\"\n",
    "    ],\n",
    "    \"died in\": [\n",
    "        \"http://dbpedia.org/property/deathPlace\",\n",
    "        \"http://dbpedia.org/ontology/deathPlace\",\n",
    "        \"http://dbpedia.org/property/placeOfDeath\"\n",
    "    ],\n",
    "    \"is set in\": [\n",
    "        \"http://dbpedia.org/ontology/location\",\n",
    "        \"http://dbpedia.org/property/location\",\n",
    "        \"http://dbpedia.org/ontology/wikiPageWikiLink\",\n",
    "        \"http://dbpedia.org/property/setting\"\n",
    "    ],\n",
    "    \"is the capital of\": [\n",
    "        \"http://dbpedia.org/ontology/capital\",\n",
    "        \"http://dbpedia.org/property/capital\",\n",
    "        \"http://dbpedia.org/ontology/capitalOf\",\n",
    "        \"http://dbpedia.org/property/capitalCity\"\n",
    "    ],\n",
    "    \"was founded in\": [\n",
    "        \"http://dbpedia.org/ontology/foundingLocation\",\n",
    "        \"http://dbpedia.org/property/foundingPlace\"\n",
    "    ],\n",
    "    \"is located in\": [\n",
    "        \"http://dbpedia.org/ontology/location\",\n",
    "        \"http://dbpedia.org/property/location\",\n",
    "        \"http://dbpedia.org/property/locationCity\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def find_best_relation_match(relation_text):\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for key in RELATION_MAP.keys():\n",
    "        score = fuzz.ratio(relation_text.lower(), key.lower())\n",
    "        if score > best_score and score > 70:  # Seuil de similarité\n",
    "            best_score = score\n",
    "            best_match = key\n",
    "    \n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "import requests\n",
    "\n",
    "def find_subject_uri(subject_text):\n",
    "    \"\"\"\n",
    "    Amélioration avec DBpedia Lookup API\n",
    "    \"\"\"\n",
    "    clean_text = subject_text.strip()\n",
    "    lookup_url = f\"https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString={quote(clean_text)}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(lookup_url, headers={'Accept': 'application/json'})\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('results', [])\n",
    "            return [r['uri'] for r in results if r.get('uri')]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback à la méthode originale\n",
    "    sub_clean = subject_text.lower().replace(\" \", \"_\")\n",
    "    return [uri for uri in data_json.keys() if sub_clean in uri.lower()]\n",
    "\n",
    "def find_object_uri(object_text, context=None):\n",
    "    \"\"\"\n",
    "    Version améliorée avec gestion du contexte\n",
    "    \"\"\"\n",
    "    obj_clean = object_text.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    # Dictionnaire de correspondances directes\n",
    "    direct_matches = {\n",
    "        \"paris\": [\"http://dbpedia.org/resource/Paris\"],\n",
    "        \"france\": [\"http://dbpedia.org/resource/France\"],\n",
    "        # Ajouter d'autres correspondances courantes\n",
    "    }\n",
    "    \n",
    "    if obj_clean in direct_matches:\n",
    "        return direct_matches[obj_clean]\n",
    "    \n",
    "    # Utiliser DBpedia Lookup comme fallback\n",
    "    try:\n",
    "        lookup_url = f\"https://lookup.dbpedia.org/api/search/KeywordSearch?QueryString={quote(object_text)}\"\n",
    "        response = requests.get(lookup_url, headers={'Accept': 'application/json'})\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('results', [])\n",
    "            return [r['uri'] for r in results if r.get('uri')]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Dernier recours : recherche dans data_json\n",
    "    return [uri for uri in data_json.keys() if obj_clean in uri.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dans ce notebook:\n",
      "\n",
      "1) On a chargé un fichier JSON local (sparql_2025-01-23_17-04-11Z.json) \n",
      "   contenant des ressources DBpedia mentionnant 'Paris'.\n",
      "\n",
      "2) On a défini un pipeline simple de NLP (extraction naive de triplets).\n",
      "3) On a illustré comment parcourir le JSON en Python pour trouver des \n",
      "   triplets du style (sujet, predicate, Paris).\n",
      "\n",
      "Pour aller plus loin, on pourrait :\n",
      "- Implémenter une désambiguïsation du subject => <http://dbpedia.org/resource/...>\n",
      "- Chercher d'autres villes que Paris\n",
      "- Ajouter une API Flask ou intégrer du SPARQLWrapper vers un endpoint local\n",
      "\n",
      "Fin de démonstration !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 7: Commentaire final\n",
    "################################################################################\n",
    "\n",
    "print(\"\"\"\n",
    "Dans ce notebook:\n",
    "\n",
    "1) On a chargé un fichier JSON local (sparql_2025-01-23_17-04-11Z.json) \n",
    "   contenant des ressources DBpedia mentionnant 'Paris'.\n",
    "\n",
    "2) On a défini un pipeline simple de NLP (extraction naive de triplets).\n",
    "3) On a illustré comment parcourir le JSON en Python pour trouver des \n",
    "   triplets du style (sujet, predicate, Paris).\n",
    "\n",
    "Pour aller plus loin, on pourrait :\n",
    "- Implémenter une désambiguïsation du subject => <http://dbpedia.org/resource/...>\n",
    "- Chercher d'autres villes que Paris\n",
    "- Ajouter une API Flask ou intégrer du SPARQLWrapper vers un endpoint local\n",
    "\n",
    "Fin de démonstration !\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fact(statement):\n",
    "    \"\"\"\n",
    "    Version améliorée avec gestion d'erreurs et scoring\n",
    "    \"\"\"\n",
    "    try:\n",
    "        trip = extract_triplet_simple(statement)\n",
    "        if not trip:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Impossible d'extraire un triplet\",\n",
    "                \"statement\": statement,\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        subject_np, rel_text, object_np = trip\n",
    "        \n",
    "        # Utiliser le fuzzy matching pour la relation\n",
    "        matched_rel_key = find_best_relation_match(rel_text)\n",
    "        \n",
    "        if not matched_rel_key:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Relation non reconnue\",\n",
    "                \"statement\": statement,\n",
    "                \"extracted_triplet\": trip,\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        dbpedia_props = RELATION_MAP[matched_rel_key]\n",
    "        subj_uris = find_subject_uri(subject_np)\n",
    "        obj_uris = find_object_uri(object_np)\n",
    "        \n",
    "        if not subj_uris or not obj_uris:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"URI non trouvé pour {subject_np if not subj_uris else object_np}\",\n",
    "                \"statement\": statement,\n",
    "                \"extracted_triplet\": trip,\n",
    "                \"relation_found\": matched_rel_key,\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Vérification avec scoring\n",
    "        max_confidence = 0.0\n",
    "        best_match = None\n",
    "        \n",
    "        for s_uri in subj_uris:\n",
    "            if s_uri in data_json:\n",
    "                predicates_dict = data_json[s_uri]\n",
    "                for prop in dbpedia_props:\n",
    "                    if prop in predicates_dict:\n",
    "                        for obj_val in predicates_dict[prop]:\n",
    "                            if obj_val[\"value\"] in obj_uris:\n",
    "                                confidence = 1.0  # Score parfait pour une correspondance exacte\n",
    "                                if confidence > max_confidence:\n",
    "                                    max_confidence = confidence\n",
    "                                    best_match = (s_uri, prop, obj_val[\"value\"])\n",
    "        \n",
    "        if best_match:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": True,\n",
    "                \"message\": f\"Fact found: {best_match[0]} {best_match[1]} => {best_match[2]}\",\n",
    "                \"extracted_triplet\": trip,\n",
    "                \"relation_found\": matched_rel_key,\n",
    "                \"confidence\": max_confidence\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"result\": False,\n",
    "            \"message\": \"Aucun triple correspondant trouvé\",\n",
    "            \"extracted_triplet\": trip,\n",
    "            \"relation_found\": matched_rel_key,\n",
    "            \"confidence\": 0.0\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Erreur inattendue: {str(e)}\",\n",
    "            \"statement\": statement,\n",
    "            \"confidence\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: Madame Roland was born in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Madame_Roland http://dbpedia.org/property/birthPlace => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Madame Roland', 'born in', 'Paris'), 'relation_found': 'was born in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Jacques Hébert lived in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Jacques_Hébert http://dbpedia.org/ontology/residence => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Jacques Hébert', 'lived in', 'Paris'), 'relation_found': 'lived in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Heartland is set in Paris\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Heartland is set in Paris', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Paris was the capital of France\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Paris was the capital of France', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Voltaire died in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Voltaire http://dbpedia.org/ontology/deathPlace => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Voltaire', 'died in', 'Paris'), 'relation_found': 'died in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Louis Figo was born in Paris\n",
      "Result: {'success': False, 'error': 'URI non trouvé pour Louis Figo', 'statement': 'Louis Figo was born in Paris', 'extracted_triplet': ('Louis Figo', 'born in', 'Paris'), 'relation_found': 'was born in', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Auguste Rodin was born in Paris\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Auguste Rodin was born in Paris', 'confidence': 0.0}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 8: Quelques questions d'exemple\n",
    "################################################################################\n",
    "\n",
    "statements = [\n",
    "    \"Madame Roland was born in Paris\",        # Dans ton JSON: Madame_Roland => birthPlace => Paris\n",
    "    \"Jacques Hébert lived in Paris\",          # Jacques_Hébert => residence => Paris\n",
    "    \"Heartland is set in Paris\",              # Heartland => wikiPageWikiLink => Paris\n",
    "    \"Paris was the capital of France\",        # On n'a pas la triple capital => ?\n",
    "    \"Voltaire died in Paris\",                 # Voltaire => deathPlace => Paris\n",
    "    \"Louis Figo was born in Paris\",           # test d'erreur, car Figo n'est pas né à Paris\n",
    "    \"Auguste Rodin was born in Paris\"         # Rodin => birthPlace => Paris\n",
    "]\n",
    "\n",
    "for st in statements:\n",
    "    result = check_fact(st)\n",
    "    print(\"Statement:\", st)\n",
    "    print(\"Result:\", result)\n",
    "    print(\"--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: Madame Roland was born in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Madame_Roland http://dbpedia.org/property/birthPlace => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Madame Roland', 'born in', 'Paris'), 'relation_found': 'was born in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Jacques Hébert lived in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Jacques_Hébert http://dbpedia.org/ontology/residence => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Jacques Hébert', 'lived in', 'Paris'), 'relation_found': 'lived in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Heartland is set in Paris\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Heartland is set in Paris', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Paris was the capital of France\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Paris was the capital of France', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Voltaire died in Paris\n",
      "Result: {'success': True, 'result': True, 'message': 'Fact found: http://dbpedia.org/resource/Voltaire http://dbpedia.org/ontology/deathPlace => http://dbpedia.org/resource/Paris', 'extracted_triplet': ('Voltaire', 'died in', 'Paris'), 'relation_found': 'died in', 'confidence': 1.0}\n",
      "--------\n",
      "Statement: Louis Figo was born in Paris\n",
      "Result: {'success': False, 'error': 'URI non trouvé pour Louis Figo', 'statement': 'Louis Figo was born in Paris', 'extracted_triplet': ('Louis Figo', 'born in', 'Paris'), 'relation_found': 'was born in', 'confidence': 0.0}\n",
      "--------\n",
      "Statement: Auguste Rodin was born in Paris\n",
      "Result: {'success': False, 'error': \"Impossible d'extraire un triplet\", 'statement': 'Auguste Rodin was born in Paris', 'confidence': 0.0}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 8: Quelques questions d'exemple\n",
    "################################################################################\n",
    "\n",
    "statements = [\n",
    "    \"Madame Roland was born in Paris\",        # Dans ton JSON: Madame_Roland => birthPlace => Paris\n",
    "    \"Jacques Hébert lived in Paris\",          # Jacques_Hébert => residence => Paris\n",
    "    \"Heartland is set in Paris\",              # Heartland => wikiPageWikiLink => Paris\n",
    "    \"Paris was the capital of France\",        # On n'a pas la triple capital => ?\n",
    "    \"Voltaire died in Paris\",                 # Voltaire => deathPlace => Paris\n",
    "    \"Louis Figo was born in Paris\",           # test d'erreur, car Figo n'est pas né à Paris\n",
    "    \"Auguste Rodin was born in Paris\"         # Rodin => birthPlace => Paris\n",
    "]\n",
    "\n",
    "for st in statements:\n",
    "    result = check_fact(st)\n",
    "    print(\"Statement:\", st)\n",
    "    print(\"Result:\", result)\n",
    "    print(\"--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCe code montre un pipeline plus robuste que la version minimale:\\n- On a un RELATION_MAP pour faire le lien phrase => propriété(s) DBpedia\\n- On a un naive subject/object matching (ex. \"Madame Roland\" => \"http://dbpedia.org/resource/Madame_Roland\")\\n- On check si la triple (subjectURI, propertyURI, objectURI) existe dans le data_json\\n\\nLimites:\\n1) Le mapping de relation est artisanal et partiel.\\n2) Le matching de la forme \"Madame Roland\" => \"http://dbpedia.org/resource/Madame_Roland\" est naïf:\\n   - on fait un simple \"sub_clean in uri.lower()\".\\n3) On ne gère pas la langue, les majuscules, les homonymes, etc.\\n4) On se limite à \"Paris\" ou \"France\" pour l\\'objet => \"http://dbpedia.org/resource/Paris\"/\"France\".\\n5) On se base sur ton JSON local, qui ne contient que des mentions de \"Paris\" (et pas d\\'autres villes).\\n\\nAméliorations possibles:\\n- Utiliser un vrai parseur sémantique (spaCy / stanza) pour extraire des triplets plus fiables.\\n- Faire du fuzzy matching plus avancé via label => URI (ex. en local, on peut indexer \"Madame Roland\" => \"Madame_Roland\" si on avait rdfs:label).\\n- Charger un triple store local (Virtuoso/Fuseki) et faire des requêtes SPARQL => plus fiable, plus complet.\\n- Gérer un mapping plus large: \"capital\" => dbo:capitalCity + dbp:capital + etc.\\n\\nMais ce notebook illustre déjà un \"Automated Fact Checking\" en version simplifiée, \\nadapté à ton dataset JSON mentionnant Paris.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# CELL 9: Commentaires finaux et pistes d'amélioration\n",
    "################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Ce code montre un pipeline plus robuste que la version minimale:\n",
    "- On a un RELATION_MAP pour faire le lien phrase => propriété(s) DBpedia\n",
    "- On a un naive subject/object matching (ex. \"Madame Roland\" => \"http://dbpedia.org/resource/Madame_Roland\")\n",
    "- On check si la triple (subjectURI, propertyURI, objectURI) existe dans le data_json\n",
    "\n",
    "Limites:\n",
    "1) Le mapping de relation est artisanal et partiel.\n",
    "2) Le matching de la forme \"Madame Roland\" => \"http://dbpedia.org/resource/Madame_Roland\" est naïf:\n",
    "   - on fait un simple \"sub_clean in uri.lower()\".\n",
    "3) On ne gère pas la langue, les majuscules, les homonymes, etc.\n",
    "4) On se limite à \"Paris\" ou \"France\" pour l'objet => \"http://dbpedia.org/resource/Paris\"/\"France\".\n",
    "5) On se base sur ton JSON local, qui ne contient que des mentions de \"Paris\" (et pas d'autres villes).\n",
    "\n",
    "Améliorations possibles:\n",
    "- Utiliser un vrai parseur sémantique (spaCy / stanza) pour extraire des triplets plus fiables.\n",
    "- Faire du fuzzy matching plus avancé via label => URI (ex. en local, on peut indexer \"Madame Roland\" => \"Madame_Roland\" si on avait rdfs:label).\n",
    "- Charger un triple store local (Virtuoso/Fuseki) et faire des requêtes SPARQL => plus fiable, plus complet.\n",
    "- Gérer un mapping plus large: \"capital\" => dbo:capitalCity + dbp:capital + etc.\n",
    "\n",
    "Mais ce notebook illustre déjà un \"Automated Fact Checking\" en version simplifiée, \n",
    "adapté à ton dataset JSON mentionnant Paris.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le serveur Flask est démarré sur http://localhost:5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\n\\n# Test de la route get-sample-questions\\nresponse = requests.get(\\'http://localhost:5000/get-sample-questions\\')\\nprint(\"Sample Questions Response:\", response.json())\\n\\n# Test de la route verify-fact\\ntest_data = {\"statement\": \"Voltaire died in Paris\"}\\nresponse = requests.post(\\'http://localhost:5000/verify-fact\\', json=test_data)\\nprint(\"\\nVerify Fact Response:\", response.json())\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [23/Jan/2025 20:07:54] \"OPTIONS /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:07:54] \"POST /verify-fact HTTP/1.1\" 400 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:08:56] \"OPTIONS /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:08:56] \"POST /verify-fact HTTP/1.1\" 400 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:09:00] \"GET /get-sample-questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:09:41] \"OPTIONS /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:09:44] \"POST /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:35] \"GET /get-sample-questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:37] \"OPTIONS /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:37] \"POST /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:42] \"GET /get-sample-questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:43] \"OPTIONS /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:46] \"POST /verify-fact HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Jan/2025 20:10:52] \"GET /get-sample-questions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import random\n",
    "from threading import Thread\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "def generate_response(fact_result):\n",
    "    \"\"\"\n",
    "    Génère une réponse naturelle basée sur le résultat de la vérification\n",
    "    \"\"\"\n",
    "    if not fact_result['success']:\n",
    "        error_responses = [\n",
    "            \"Je suis désolé, mais je ne peux pas analyser cette phrase correctement.\",\n",
    "            \"Cette question est un peu compliquée pour moi.\",\n",
    "            \"Pourriez-vous reformuler votre question différemment ?\",\n",
    "        ]\n",
    "        return {\n",
    "            \"natural_response\": random.choice(error_responses),\n",
    "            \"details\": fact_result\n",
    "        }\n",
    "\n",
    "    subject, relation, object_ = fact_result.get('extracted_triplet', ('', '', ''))\n",
    "    confidence = fact_result.get('confidence', 0.0)\n",
    "\n",
    "    if fact_result['result']:\n",
    "        # Réponses positives\n",
    "        positive_responses = [\n",
    "            f\"Oui, c'est exact ! {subject} {relation} {object_}.\",\n",
    "            f\"En effet, j'ai trouvé cette information dans mes données.\",\n",
    "            f\"C'est correct ! Cette information est vérifiée.\",\n",
    "        ]\n",
    "        response = random.choice(positive_responses)\n",
    "        \n",
    "        # Ajouter des détails si disponibles\n",
    "        if fact_result.get('message'):\n",
    "            response += f\"\\nCette information provient de DBpedia.\"\n",
    "            \n",
    "    else:\n",
    "        # Réponses négatives\n",
    "        negative_responses = [\n",
    "            f\"Je ne peux pas confirmer que {subject} {relation} {object_}.\",\n",
    "            \"Je n'ai pas trouvé cette information dans mes données.\",\n",
    "            \"Cette affirmation ne semble pas être correcte selon mes sources.\",\n",
    "        ]\n",
    "        response = random.choice(negative_responses)\n",
    "        \n",
    "        # Suggérer une correction si possible\n",
    "        if fact_result.get('alternative_facts'):\n",
    "            response += f\"\\nCependant, voici ce que je sais : {fact_result['alternative_facts']}\"\n",
    "\n",
    "    return {\n",
    "        \"natural_response\": response,\n",
    "        \"details\": fact_result\n",
    "    }\n",
    "\n",
    "@app.route('/verify-fact', methods=['POST'])\n",
    "def verify_fact():\n",
    "    \"\"\"\n",
    "    Route pour vérifier un fait et générer une réponse naturelle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        statement = data.get('statement')\n",
    "        \n",
    "        if not statement:\n",
    "            return jsonify({\n",
    "                \"error\": \"Aucune question fournie\",\n",
    "                \"success\": False\n",
    "            }), 400\n",
    "\n",
    "        # Vérifier le fait\n",
    "        fact_result = check_fact(statement)\n",
    "        \n",
    "        # Générer une réponse naturelle\n",
    "        response = generate_response(fact_result)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"success\": True,\n",
    "            \"response\": response[\"natural_response\"],\n",
    "            \"verification_details\": response[\"details\"],\n",
    "            \"extracted_information\": {\n",
    "                \"triplet\": fact_result.get('extracted_triplet'),\n",
    "                \"confidence\": fact_result.get('confidence'),\n",
    "                \"relation_found\": fact_result.get('relation_found')\n",
    "            }\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }), 500\n",
    "\n",
    "@app.route('/get-sample-questions', methods=['GET'])\n",
    "def get_sample_questions():\n",
    "    \"\"\"\n",
    "    Retourne une liste de questions exemple\n",
    "    \"\"\"\n",
    "    sample_questions = [\n",
    "        \"Madame Roland was born in Paris\",\n",
    "        \"Voltaire died in Paris\",\n",
    "        \"Auguste Rodin was born in Paris\",\n",
    "        \"Paris was the capital of France\",\n",
    "        \"Jacques Hébert lived in Paris\"\n",
    "    ]\n",
    "    return jsonify({\n",
    "        \"success\": True,\n",
    "        \"sample_questions\": sample_questions\n",
    "    })\n",
    "\n",
    "def run_flask():\n",
    "    \"\"\"\n",
    "    Fonction pour exécuter Flask dans un thread séparé\n",
    "    \"\"\"\n",
    "    app.run(debug=False, port=5000, use_reloader=False)\n",
    "\n",
    "# Démarrer Flask dans un thread séparé\n",
    "flask_thread = Thread(target=run_flask)\n",
    "flask_thread.daemon = True  # Le thread s'arrêtera quand le notebook sera fermé\n",
    "flask_thread.start()\n",
    "\n",
    "print(\"Le serveur Flask est démarré sur http://localhost:5000\")\n",
    "\n",
    "# Cellule de test (à exécuter dans une cellule séparée)\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "# Test de la route get-sample-questions\n",
    "response = requests.get('http://localhost:5000/get-sample-questions')\n",
    "print(\"Sample Questions Response:\", response.json())\n",
    "\n",
    "# Test de la route verify-fact\n",
    "test_data = {\"statement\": \"Voltaire died in Paris\"}\n",
    "response = requests.post('http://localhost:5000/verify-fact', json=test_data)\n",
    "print(\"\\nVerify Fact Response:\", response.json())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "os.kill(os.getpid(), signal.SIGINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
